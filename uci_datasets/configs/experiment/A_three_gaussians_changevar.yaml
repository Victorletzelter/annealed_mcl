# @package _global_

# to execute this experiment run:
# python train.py experiment=example

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

seed: 12345

early_stopping:
  patience: 1000 # number of checks with no improvement after which training will be stopped

model:
  input_dim: 1
  output_dim: 2
  restrict_to_square: True
  num_hypothesis: 20
  hparams:
    plot_mode: True
    plot_mode_training: True
    training_wta_mode: "stable_awta"
    plot_training_frequency: 10
    learning_rate: 0.01
    temperature_ini: 0.3
    scheduler_mode: "linear"
    temperature_decay: 0.9

trainer:
  accelerator: gpu
  fast_dev_run: False
  max_epochs: 1
  check_val_every_n_epoch: 50
  limit_train_batches: 1.0
  limit_val_batches: 1.0
  limit_test_batches: 1.0

hydra:
  job:
    name: "annealing_three_gaussians"
  run:
    dir: ${paths.log_dir}/${task_name}/setup_three_gaussians_changevar_${trainer.max_epochs}/_${now:%Y-%m-%d}_${now:%H-%M-%S}_${hydra:job.name}
    
data:
  batch_size: 65536
  hparams:
    name: "three_gaussians_changevar"
    n_samples_train: 100000
    n_samples_val: 25000

logger:
  mlflow:
    experiment_name: "three_gaussians_changevar_setup"
    run_name: ${hydra:job.name} 