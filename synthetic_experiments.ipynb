{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import matplotlib.animation\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCL_MSE(nn.Module):\n",
    "\n",
    "    def __init__(self, return_assigmnent=True, epsilon=1e-9):\n",
    "        super(MCL_MSE, self).__init__()\n",
    "        self.return_assignment = return_assigmnent\n",
    "        self.epsilon = self.EPS = epsilon\n",
    "\n",
    "    def forward(self, prediction_list, score_list, target_list, epoch=None):\n",
    "\n",
    "        # extract shape\n",
    "        n_prediction, n_target = prediction_list.shape[1], target_list.shape[1]\n",
    "\n",
    "        # compute pairwise distance\n",
    "        pairwise_distance = -torch.square(prediction_list.unsqueeze(2)-target_list.unsqueeze(1)).sum(dim=-1)\n",
    "\n",
    "        # assign source to closest prediction    \n",
    "        mcl_sisdr, target_assignment = pairwise_distance.max(dim=1) # [batch_size, n_target], [batch_size, n_target]\n",
    "        mcl_sisdr = -mcl_sisdr.mean() # []\n",
    "\n",
    "        # compute prediction -> target assignment\n",
    "        if self.return_assignment:\n",
    "            prediction_assignment = torch.stack([torch.nn.functional.one_hot(target_assignment[:,target_index], num_classes=n_prediction).float() for target_index in range(n_target)], dim=-1) # [batch_size, n_prediction, n_target]\n",
    "            score_loss = self.score_metric(score_list, prediction_assignment.any(dim=-1).float()) # []\n",
    "\n",
    "        return (mcl_sisdr, score_loss, prediction_assignment) if (self.return_score_loss and self.return_assignment) else  ((mcl_sisdr, score_loss) if self.return_score_loss else mcl_sisdr)\n",
    "\n",
    "class Annealed_MCL_MSE(nn.Module):\n",
    "    def __init__(self, temperature_schedule, return_score_loss=True, return_assignment=False, sample_normalization=True, epsilon=1e-9, min_temperature=1e-4):\n",
    "        super(Annealed_MCL_MSE, self).__init__()\n",
    "        self.score_metric = torch.nn.BCELoss()\n",
    "        self.return_score_loss = return_score_loss\n",
    "        self.return_assignment = return_assignment\n",
    "        self.sample_normalization = sample_normalization\n",
    "        self.epsilon = epsilon\n",
    "        self.temperature_schedule = temperature_schedule\n",
    "        self.min_temperature = min_temperature\n",
    "\n",
    "    def forward(self, prediction_list, score_list, target_list, epoch=None):\n",
    "        # extract shape\n",
    "        n_prediction, n_target = prediction_list.shape[1], target_list.shape[1]\n",
    "\n",
    "        # compute pairwise distance\n",
    "        pairwise_distance = -torch.square(prediction_list.unsqueeze(1)-target_list.unsqueeze(2)).sum(dim=-1) # [batch_size, n_target, n_prediction]\n",
    "\n",
    "        # soft assignation of source to closest prediction (& hard assignment for scoring purposes)\n",
    "        temperature = self.temperature_schedule(epoch)\n",
    "        amcl_sisdr = (torch.softmax(pairwise_distance / temperature, dim=2).detach() * pairwise_distance).sum(dim=2)  if temperature > self.min_temperature else pairwise_distance.max(dim=2)[0] # [batch_size, n_target]  \n",
    "        target_assignment = pairwise_distance.max(dim=2)[1] # [batch_size, n_target]\n",
    "\n",
    "        # mask inactive target (normalize per sample)\n",
    "        target_mask = (target_list.abs().sum(dim=-1) > 0.).squeeze(-1) # [batch_size, n_target]\n",
    "        amcl_sisdr = ((amcl_sisdr * target_mask).sum(dim=-1) / target_mask.sum(dim=-1)).mean() if self.sample_normalization else amcl_sisdr[target_mask].mean() # []\n",
    "        amcl_sisdr = - amcl_sisdr\n",
    "\n",
    "        # compute prediction -> target assignment\n",
    "        if True:\n",
    "            prediction_assignment = torch.stack([torch.nn.functional.one_hot(target_assignment[:,target_index], num_classes=n_prediction).float() for target_index in range(n_target)], dim=-1) # [batch_size, n_prediction, n_target]\n",
    "            score_loss = self.score_metric(score_list, prediction_assignment.any(dim=-1).float()) # []\n",
    "        return (amcl_sisdr, score_loss, prediction_assignment) if (self.return_score_loss and self.return_assignment) else  ((amcl_sisdr, score_loss) if self.return_score_loss else amcl_sisdr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla MCL (annealed MCL with no temperature scheduler)\n",
    "We fit the hypotheses to a synthetic dataset with 3 gaussian modes. This first example presents the collapse issue with vanilla MCL: most of the hypotheses converge to the barycenter of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "\n",
    "# data parameters\n",
    "input_dim = 2\n",
    "# training set generated at he beginning \n",
    "len_training = 1\n",
    "n_epoch = 20\n",
    "device = \"cpu\"\n",
    "\n",
    "# training set\n",
    "loc = torch.Tensor([-2,2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn1 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "loc = torch.Tensor([0,-2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn2 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "loc = torch.Tensor([2,2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn3 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "mvn_set = [mvn1,mvn2,mvn3]\n",
    "\n",
    "batch_size = 1000\n",
    "# batch with 3 2-D gaussians with different mean and covariance\n",
    "target_list = torch.cat([mvn_set[i].sample((batch_size,)) for i in range(3)],dim=0).unsqueeze(1).to(device)\n",
    "\n",
    "\n",
    "# model parameters\n",
    "n_model = 10 #number of hypotheses\n",
    "hidden_dim=128\n",
    "\n",
    "# temperature schedule set to 0\n",
    "temperature_schedule = lambda epoch : 0\n",
    "training_metric = Annealed_MCL_MSE(temperature_schedule=temperature_schedule)\n",
    "# training_metric = MCL_MSE(return_assigmnent=False)\n",
    "\n",
    "# model\n",
    "predictor_list = [torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, hidden_dim),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim, input_dim),\n",
    ").to(device) for _ in range(n_model)]\n",
    "score_model_list = [torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Flatten(start_dim=0),\n",
    ").to(device) for _ in range(n_model)]\n",
    "\n",
    "# optimizer\n",
    "parameter_list = []\n",
    "for predictor, score_model in zip(predictor_list, score_model_list):\n",
    "    parameter_list += list(predictor.parameters())\n",
    "    parameter_list += list(score_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameter_list, 1e-3, betas=(0.9, 0.999))\n",
    "\n",
    "\n",
    "\n",
    "log_dict = defaultdict(list)\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    for _ in range(len_training):\n",
    "        #target_list = torch.normal(mean, variance, (batch_size, n_src, input_dim)).to(device)\n",
    "        x = torch.zeros((target_list.shape[0], input_dim)).to(device)\n",
    "        prediction_list = torch.stack([predictor(x) for predictor in predictor_list], dim=1)\n",
    "        score_list = torch.stack([score_model(x) for score_model in score_model_list], dim=1)\n",
    "        prediction_loss, score_loss = training_metric(prediction_list, score_list, target_list, epoch=epoch)\n",
    "        # prediction_loss, score_loss = training_metric(prediction_list, target_list, epoch=epoch), torch.tensor(0)\n",
    "        loss = prediction_loss + 0 * score_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_dict[f\"loss/{epoch}\"].append(loss.item())\n",
    "        log_dict[f\"prediction_loss/{epoch}\"].append(prediction_loss.item())\n",
    "        log_dict[f\"score_loss/{epoch}\"].append(score_loss.item())\n",
    "    log_dict[f\"prediction_list/{epoch}\"].append(prediction_list[0, :, :].detach().cpu())\n",
    "    log_dict[f\"target_list/{epoch}\"].append(target_list[:, 0, :].detach().cpu())\n",
    "    log_dict[f\"temperature/{epoch}\"].append(temperature_schedule(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Voronoi\n",
    "output_data = np.stack([log_dict[f\"target_list/{epoch}\"][0] for epoch in range(n_epoch)], axis=0)\n",
    "prediction_list = np.stack([log_dict[f\"prediction_list/{epoch}\"][0] for epoch in range(n_epoch)], axis=0)\n",
    "x_min = y_min = np.min(output_data)\n",
    "x_max = y_max = np.max(output_data)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.scatter(output_data[i, :, 0], output_data[i, :, 1], label=\"data\", s=50, c=\"red\")\n",
    "    voronoi = Voronoi(prediction_list[i, :, :])\n",
    "    voronoi_plot_2d(voronoi, show_vertices=False, ax=ax, point_size=10)\n",
    "    ax.set_xlim([x_min-0.1, x_max+0.1])\n",
    "    ax.set_ylim([y_min-0.1, y_max+0.1])\n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, frames=n_epoch)\n",
    "animation.save(f'./mcl.gif', writer='imagemagick', fps=10)\n",
    "display(HTML(animation.to_jshtml()));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annealed MCL \n",
    "In this example, a temperature scheduler is added (Annealed MCL). The example shows that the hypotheses converge to each cluster with no collapse issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment parameters\n",
    "\n",
    "# data parameters\n",
    "input_dim = 2\n",
    "len_training = 1\n",
    "n_src = 1\n",
    "device = \"cpu\"\n",
    "# training set\n",
    "loc = torch.Tensor([-2,2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn1 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "loc = torch.Tensor([0,-2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn2 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "loc = torch.Tensor([2,2])\n",
    "scale = torch.ones(2)*0.1\n",
    "mvn3 = torch.distributions.MultivariateNormal(loc=loc,scale_tril=torch.diag(scale))\n",
    "\n",
    "mvn_set = [mvn1,mvn2,mvn3]\n",
    "\n",
    "batch_size = 1000\n",
    "# batch with 3 2-D gaussians with different mean and covariance\n",
    "target_list = torch.cat([mvn_set[i].sample((batch_size,)) for i in range(3)],dim=0).unsqueeze(1).to(device)\n",
    "\n",
    "# model parameters\n",
    "n_model = 30 #number of hypotheses\n",
    "hidden_dim=128\n",
    "\n",
    "# training parameters\n",
    "batch_size = 2000\n",
    "n_epoch = 200\n",
    "\n",
    "temperature_start, max_epoch, threshold = 3, 80, 5.0 # 0.025, 2*variance-1e-2\n",
    "temperature_schedule = lambda epoch : max(threshold, (temperature_start * (max_epoch - epoch) / max_epoch))\n",
    "#temperature_schedule = lambda epoch : 0\n",
    "training_metric = Annealed_MCL_MSE(temperature_schedule=temperature_schedule)\n",
    "# training_metric = MCL_MSE(return_assigmnent=False)\n",
    "\n",
    "# model\n",
    "predictor_list = [torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, hidden_dim),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(hidden_dim, input_dim),\n",
    ").to(device) for _ in range(n_model)]\n",
    "score_model_list = [torch.nn.Sequential(\n",
    "    torch.nn.Linear(input_dim, 1),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Flatten(start_dim=0),\n",
    ").to(device) for _ in range(n_model)]\n",
    "\n",
    "# optimizer\n",
    "parameter_list = []\n",
    "for predictor, score_model in zip(predictor_list, score_model_list):\n",
    "    parameter_list += list(predictor.parameters())\n",
    "    parameter_list += list(score_model.parameters())\n",
    "optimizer = torch.optim.Adam(parameter_list, 1e-3, betas=(0.9, 0.999))\n",
    "\n",
    "# predefine data\n",
    "target_list = torch.cat([mvn_set[i].sample((batch_size,)) for i in range(3)],dim=0).unsqueeze(1).to(device)\n",
    "\n",
    "log_dict = defaultdict(list)\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    for _ in range(len_training):\n",
    "        #target_list = torch.normal(mean, variance, (batch_size, n_src, input_dim)).to(device)\n",
    "        x = torch.zeros((target_list.shape[0], input_dim)).to(device)\n",
    "        prediction_list = torch.stack([predictor(x) for predictor in predictor_list], dim=1)\n",
    "        score_list = torch.stack([score_model(x) for score_model in score_model_list], dim=1)\n",
    "        prediction_loss, score_loss = training_metric(prediction_list, score_list, target_list, epoch=epoch)\n",
    "        # prediction_loss, score_loss = training_metric(prediction_list, target_list, epoch=epoch), torch.tensor(0)\n",
    "        loss = prediction_loss + 0 * score_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        log_dict[f\"loss/{epoch}\"].append(loss.item())\n",
    "        log_dict[f\"prediction_loss/{epoch}\"].append(prediction_loss.item())\n",
    "        log_dict[f\"score_loss/{epoch}\"].append(score_loss.item())\n",
    "    log_dict[f\"prediction_list/{epoch}\"].append(prediction_list[0, :, :].detach().cpu())\n",
    "    log_dict[f\"target_list/{epoch}\"].append(target_list[:, 0, :].detach().cpu())\n",
    "    log_dict[f\"temperature/{epoch}\"].append(temperature_schedule(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Voronoi\n",
    "output_data = np.stack([log_dict[f\"target_list/{epoch}\"][0] for epoch in range(n_epoch)], axis=0)\n",
    "prediction_list = np.stack([log_dict[f\"prediction_list/{epoch}\"][0] for epoch in range(n_epoch)], axis=0)\n",
    "x_min = y_min = np.min(output_data)\n",
    "x_max = y_max = np.max(output_data)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "def animate(i):\n",
    "    ax.cla()\n",
    "    ax.scatter(output_data[i, :, 0], output_data[i, :, 1], label=\"data\", s=50, c=\"red\")\n",
    "    voronoi = Voronoi(prediction_list[i, :, :])\n",
    "    voronoi_plot_2d(voronoi, show_vertices=False, ax=ax, point_size=10)\n",
    "    ax.set_xlim([x_min-0.1, x_max+0.1])\n",
    "    ax.set_ylim([y_min-0.1, y_max+0.1])\n",
    "animation = matplotlib.animation.FuncAnimation(fig, animate, frames=n_epoch)\n",
    "animation.save(f'./amcl.gif', writer='imagemagick', fps=10)\n",
    "display(HTML(animation.to_jshtml()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
